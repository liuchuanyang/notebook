# ç¼–å†™è‡ªå·±çš„GNNæ¨¡å—

æœ‰æ—¶ï¼Œæ‚¨çš„æ¨¡å‹ä¸ä»…ä»…æ˜¯ç®€å•åœ°å †å ç°æœ‰çš„ GNN æ¨¡å—ã€‚ ä¾‹å¦‚ï¼Œæ‚¨æƒ³å‘æ˜ä¸€ç§é€šè¿‡è€ƒè™‘èŠ‚ç‚¹é‡è¦æ€§æˆ–è¾¹æƒé‡æ¥èšåˆé‚»å±…ä¿¡æ¯çš„æ–°æ–¹æ³•ã€‚

åœ¨æœ¬æ•™ç¨‹ç»“æŸæ—¶ï¼Œæ‚¨å°†èƒ½å¤Ÿ

+ äº†è§£ DGL çš„æ¶ˆæ¯ä¼ é€’ APIã€‚
+ è‡ªå·±å®ç°GraphSAGEå·ç§¯æ¨¡å—ã€‚

æœ¬æ•™ç¨‹å‡è®¾æ‚¨å·²ç»äº†è§£[è®­ç»ƒç”¨äºèŠ‚ç‚¹åˆ†ç±»çš„ GNN çš„åŸºç¡€çŸ¥è¯†](https://docs.dgl.ai/tutorials/blitz/1_introduction.html)ã€‚

ï¼ˆæ—¶é—´ä¼°è®¡ï¼š10åˆ†é’Ÿï¼‰

```python
import dgl
import torch
import torch.nn as nn
import torch.nn.functional as F
```

## æ¶ˆæ¯ä¼ é€’å’Œ GNNs

DGL éµå¾ªç”± [Gilmer ç­‰äºº](https://arxiv.org/abs/1704.01212)æå‡ºçš„æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œå¯å‘çš„æ¶ˆæ¯ä¼ é€’èŒƒå¼ã€‚ ä»æœ¬è´¨ä¸Šè®²ï¼Œä»–ä»¬å‘ç°è®¸å¤š GNN æ¨¡å‹å¯ä»¥é€‚åˆä»¥ä¸‹æ¡†æ¶ï¼š

![image-20211125151031126](/Users/huan/Library/Application Support/typora-user-images/image-20211125151031126.png)
$$
M^{(l)}è¡¨ç¤ºæ¶ˆæ¯ä¼ é€’å‡½æ•°ï¼Œ\sum è¡¨ç¤ºèšåˆå‡½æ•°ï¼ŒU^{(l)}è¡¨ç¤ºæ›´æ–°å‡½æ•°ï¼›
æç¤º\sumè¡¨ç¤ºä¸€ä¸ªå‡½æ•°ï¼Œå¹¶ä¸ä¸€å®šæ˜¯æ±‚å’Œ
$$
ä¾‹å¦‚ï¼Œ[GraphSAGE å·ç§¯ï¼ˆHamilton ç­‰äººï¼Œ2017 å¹´ï¼‰](https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf)é‡‡ç”¨ä»¥ä¸‹æ•°å­¦å½¢å¼ï¼š

![image-20211125151637658](/Users/huan/Library/Application Support/typora-user-images/image-20211125151637658.png)

ä½ å¯ä»¥çœ‹åˆ°æ¶ˆæ¯ä¼ é€’æ˜¯æœ‰æ–¹å‘çš„ï¼šä»ä¸€ä¸ªèŠ‚ç‚¹ğ‘¢å‘é€åˆ°å¦ä¸€ä¸ªèŠ‚ç‚¹ğ‘£çš„æ¶ˆæ¯ä¸ä¸€å®šä¸ä»èŠ‚ç‚¹ğ‘£å‘é€åˆ°èŠ‚ç‚¹ğ‘¢çš„å¦ä¸€æ¡æ¶ˆæ¯ä»¥ç›¸åçš„æ–¹å‘ç›¸åŒã€‚



å°½ç®¡ DGL é€šè¿‡ `dgl.nn.SAGEConv` å†…ç½®äº†å¯¹ `GraphSAGE `çš„æ”¯æŒï¼Œä½†æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åœ¨ DGL ä¸­è‡ªè¡Œå®ç° GraphSAGE å·ç§¯ã€‚

```python
import dgl.function as fn

class SAGEConv(nn.Module):
    """Graph convolution module used by the GraphSAGE model.

    Parameters
    ----------
    in_feat : int
        Input feature size.
    out_feat : int
        Output feature size.
    """
    def __init__(self, in_feat, out_feat):
        super(SAGEConv, self).__init__()
        # A linear submodule for projecting the input and neighbor feature to the output.
        self.linear = nn.Linear(in_feat * 2, out_feat)

    def forward(self, g, h):
        """Forward computation

        Parameters
        ----------
        g : Graph
            The input graph.
        h : Tensor
            The input node feature.
        """
        with g.local_scope():
            g.ndata['h'] = h
            # update_all is a message passing API.
            g.update_all(message_func=fn.copy_u('h', 'm'), reduce_func=fn.mean('m', 'h_N'))
            h_N = g.ndata['h_N']
            h_total = torch.cat([h, h_N], dim=1)
            return self.linear(h_total)
```

è¿™æ®µä»£ç çš„æ ¸å¿ƒéƒ¨åˆ†æ˜¯` g.update_all `å‡½æ•°ï¼Œå®ƒæ”¶é›†å’Œå¹³å‡ç›¸é‚»ç‰¹å¾ã€‚ è¿™é‡Œæœ‰ä¸‰ä¸ªæ¦‚å¿µï¼š

+ æ¶ˆæ¯å‡½æ•° **fn.copy_u('h', 'm') **å°†åç§°ä¸º **'h'** çš„èŠ‚ç‚¹ç‰¹å¾å¤åˆ¶ä¸ºå‘é€ç»™é‚»å±…çš„æ¶ˆæ¯ã€‚
+ Reduceï¼ˆèšåˆï¼‰ å‡½æ•° **fn.mean('m', 'h_N') **å¯¹åç§°ä¸º **'m' **çš„æ‰€æœ‰æ¥æ”¶æ¶ˆæ¯è¿›è¡Œå¹³å‡ï¼Œå¹¶å°†ç»“æœä¿å­˜ä¸ºæ–°çš„èŠ‚ç‚¹ç‰¹å¾ **'h_N'**ã€‚
+ **update_all **å‘Šè¯‰ DGL ä¸ºæ‰€æœ‰èŠ‚ç‚¹å’Œè¾¹è§¦å‘***æ¶ˆæ¯***å’Œ***èšåˆå‡½æ•°***ã€‚



ä¹‹åï¼Œæ‚¨å¯ä»¥å †å è‡ªå·±çš„ GraphSAGE å·ç§¯å±‚ä»¥å½¢æˆå¤šå±‚ GraphSAGE ç½‘ç»œã€‚

```python
class Model(nn.Module):
    def __init__(self, in_feats, h_feats, num_classes):
        super(Model, self).__init__()
        self.conv1 = SAGEConv(in_feats, h_feats)
        self.conv2 = SAGEConv(h_feats, num_classes)

    def forward(self, g, in_feat):
        h = self.conv1(g, in_feat)
        h = F.relu(h)
        h = self.conv2(g, h)
        return h
```

## è®­ç»ƒ

ä»¥ä¸‹æ•°æ®åŠ è½½å’Œè®­ç»ƒå¾ªç¯çš„ä»£ç ç›´æ¥å¤åˆ¶è‡ªä»‹ç»æ•™ç¨‹ã€‚

```python
import dgl.data

dataset = dgl.data.CoraGraphDataset()
g = dataset[0]

def train(g, model):
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    all_logits = []
    best_val_acc = 0
    best_test_acc = 0

    features = g.ndata['feat']
    labels = g.ndata['label']
    train_mask = g.ndata['train_mask']
    val_mask = g.ndata['val_mask']
    test_mask = g.ndata['test_mask']
    for e in range(200):
        # Forward
        logits = model(g, features)

        # Compute prediction
        pred = logits.argmax(1)

        # Compute loss
        # Note that we should only compute the losses of the nodes in the training set,
        # i.e. with train_mask 1.
        loss = F.cross_entropy(logits[train_mask], labels[train_mask])

        # Compute accuracy on training/validation/test
        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()
        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()
        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()

        # Save the best validation accuracy and the corresponding test accuracy.
        if best_val_acc < val_acc:
            best_val_acc = val_acc
            best_test_acc = test_acc

        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        all_logits.append(logits.detach())

        if e % 5 == 0:
            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(
                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))

model = Model(g.ndata['feat'].shape[1], 16, dataset.num_classes)
train(g, model)
```



Out:

```
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
In epoch 0, loss: 1.949, val acc: 0.114 (best 0.114), test acc: 0.103 (best 0.103)
In epoch 5, loss: 1.904, val acc: 0.160 (best 0.160), test acc: 0.154 (best 0.154)
In epoch 10, loss: 1.804, val acc: 0.432 (best 0.432), test acc: 0.454 (best 0.454)
In epoch 15, loss: 1.644, val acc: 0.462 (best 0.462), test acc: 0.469 (best 0.469)
In epoch 20, loss: 1.423, val acc: 0.488 (best 0.488), test acc: 0.499 (best 0.499)
In epoch 25, loss: 1.154, val acc: 0.578 (best 0.578), test acc: 0.584 (best 0.584)
In epoch 30, loss: 0.866, val acc: 0.680 (best 0.680), test acc: 0.676 (best 0.676)
In epoch 35, loss: 0.600, val acc: 0.726 (best 0.726), test acc: 0.740 (best 0.740)
In epoch 40, loss: 0.389, val acc: 0.738 (best 0.744), test acc: 0.754 (best 0.747)
In epoch 45, loss: 0.242, val acc: 0.742 (best 0.744), test acc: 0.759 (best 0.747)
In epoch 50, loss: 0.150, val acc: 0.742 (best 0.748), test acc: 0.759 (best 0.756)
In epoch 55, loss: 0.095, val acc: 0.752 (best 0.752), test acc: 0.760 (best 0.760)
In epoch 60, loss: 0.063, val acc: 0.746 (best 0.752), test acc: 0.764 (best 0.760)
In epoch 65, loss: 0.044, val acc: 0.750 (best 0.752), test acc: 0.763 (best 0.760)
In epoch 70, loss: 0.032, val acc: 0.750 (best 0.752), test acc: 0.766 (best 0.760)
In epoch 75, loss: 0.025, val acc: 0.750 (best 0.752), test acc: 0.766 (best 0.760)
In epoch 80, loss: 0.020, val acc: 0.750 (best 0.752), test acc: 0.764 (best 0.760)
In epoch 85, loss: 0.017, val acc: 0.750 (best 0.752), test acc: 0.766 (best 0.760)
In epoch 90, loss: 0.015, val acc: 0.750 (best 0.752), test acc: 0.766 (best 0.760)
In epoch 95, loss: 0.013, val acc: 0.756 (best 0.756), test acc: 0.767 (best 0.767)
In epoch 100, loss: 0.012, val acc: 0.756 (best 0.756), test acc: 0.765 (best 0.767)
In epoch 105, loss: 0.011, val acc: 0.756 (best 0.756), test acc: 0.764 (best 0.767)
In epoch 110, loss: 0.010, val acc: 0.756 (best 0.756), test acc: 0.764 (best 0.767)
In epoch 115, loss: 0.009, val acc: 0.758 (best 0.758), test acc: 0.764 (best 0.764)
In epoch 120, loss: 0.008, val acc: 0.758 (best 0.758), test acc: 0.764 (best 0.764)
In epoch 125, loss: 0.008, val acc: 0.758 (best 0.758), test acc: 0.765 (best 0.764)
In epoch 130, loss: 0.007, val acc: 0.758 (best 0.758), test acc: 0.764 (best 0.764)
In epoch 135, loss: 0.007, val acc: 0.758 (best 0.758), test acc: 0.764 (best 0.764)
In epoch 140, loss: 0.006, val acc: 0.758 (best 0.758), test acc: 0.764 (best 0.764)
In epoch 145, loss: 0.006, val acc: 0.758 (best 0.758), test acc: 0.763 (best 0.764)
In epoch 150, loss: 0.006, val acc: 0.760 (best 0.760), test acc: 0.763 (best 0.763)
In epoch 155, loss: 0.005, val acc: 0.758 (best 0.760), test acc: 0.765 (best 0.763)
In epoch 160, loss: 0.005, val acc: 0.756 (best 0.760), test acc: 0.764 (best 0.763)
In epoch 165, loss: 0.005, val acc: 0.756 (best 0.760), test acc: 0.766 (best 0.763)
In epoch 170, loss: 0.004, val acc: 0.754 (best 0.760), test acc: 0.766 (best 0.763)
In epoch 175, loss: 0.004, val acc: 0.756 (best 0.760), test acc: 0.765 (best 0.763)
In epoch 180, loss: 0.004, val acc: 0.756 (best 0.760), test acc: 0.765 (best 0.763)
In epoch 185, loss: 0.004, val acc: 0.756 (best 0.760), test acc: 0.765 (best 0.763)
In epoch 190, loss: 0.004, val acc: 0.756 (best 0.760), test acc: 0.765 (best 0.763)
In epoch 195, loss: 0.004, val acc: 0.756 (best 0.760), test acc: 0.765 (best 0.763)
```

## æ›´å¤šè‡ªå®šä¹‰

åœ¨ DGL ä¸­ï¼Œæˆ‘ä»¬åœ¨ `dgl.function `åŒ…ä¸‹æä¾›äº†è®¸å¤šå†…ç½®çš„ message å’Œ reduce å‡½æ•°ã€‚ æ‚¨å¯ä»¥åœ¨[ API](https://docs.dgl.ai/api/python/dgl.function.html#apifunction) æ–‡æ¡£ä¸­æ‰¾åˆ°æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

è¿™äº› API å…è®¸äººä»¬å¿«é€Ÿå®ç°æ–°çš„å›¾å·ç§¯æ¨¡å—ã€‚ ä¾‹å¦‚ï¼Œä¸‹é¢å®ç°äº†ä¸€ä¸ªæ–°çš„ `SAGEConv`ï¼Œå®ƒä½¿ç”¨åŠ æƒå¹³å‡èšåˆé‚»å±…è¡¨ç¤ºã€‚ è¯·æ³¨æ„ï¼Œ `edata `æˆå‘˜å¯ä»¥ä¿å­˜è¾¹ç¼˜ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾ä¹Ÿå¯ä»¥å‚ä¸æ¶ˆæ¯ä¼ é€’ã€‚

```python
class WeightedSAGEConv(nn.Module):
    """Graph convolution module used by the GraphSAGE model with edge weights.

    Parameters
    ----------
    in_feat : int
        Input feature size.
    out_feat : int
        Output feature size.
    """
    def __init__(self, in_feat, out_feat):
        super(WeightedSAGEConv, self).__init__()
        # A linear submodule for projecting the input and neighbor feature to the output.
        self.linear = nn.Linear(in_feat * 2, out_feat)

    def forward(self, g, h, w):
        """Forward computation

        Parameters
        ----------
        g : Graph
            The input graph.
        h : Tensor
            The input node feature.
        w : Tensor
            The edge weight.
        """
        with g.local_scope():
            g.ndata['h'] = h
            g.edata['w'] = w
            g.update_all(message_func=fn.u_mul_e('h', 'w', 'm'), reduce_func=fn.mean('m', 'h_N'))
            h_N = g.ndata['h_N']
            h_total = torch.cat([h, h_N], dim=1)
            return self.linear(h_total)
```

å› ä¸ºè¿™ä¸ªæ•°æ®é›†ä¸­çš„å›¾æ²¡æœ‰è¾¹æƒé‡ï¼Œæˆ‘ä»¬åœ¨æ¨¡å‹çš„ `forward() `å‡½æ•°ä¸­æ‰‹åŠ¨å°†æ‰€æœ‰è¾¹æƒé‡åˆ†é…ä¸º1ã€‚ æ‚¨å¯ä»¥å°†å…¶æ›¿æ¢ä¸ºæ‚¨è‡ªå·±çš„è¾¹æƒé‡ã€‚

```python
class Model(nn.Module):
    def __init__(self, in_feats, h_feats, num_classes):
        super(Model, self).__init__()
        self.conv1 = WeightedSAGEConv(in_feats, h_feats)
        self.conv2 = WeightedSAGEConv(h_feats, num_classes)

    def forward(self, g, in_feat):
        h = self.conv1(g, in_feat, torch.ones(g.num_edges(), 1).to(g.device))
        h = F.relu(h)
        h = self.conv2(g, h, torch.ones(g.num_edges(), 1).to(g.device))
        return h

model = Model(g.ndata['feat'].shape[1], 16, dataset.num_classes)
train(g, model)
```

Out:

```
In epoch 0, loss: 1.949, val acc: 0.316 (best 0.316), test acc: 0.317 (best 0.317)
In epoch 5, loss: 1.886, val acc: 0.422 (best 0.422), test acc: 0.451 (best 0.451)
In epoch 10, loss: 1.756, val acc: 0.476 (best 0.532), test acc: 0.470 (best 0.524)
In epoch 15, loss: 1.556, val acc: 0.550 (best 0.550), test acc: 0.529 (best 0.529)
In epoch 20, loss: 1.293, val acc: 0.618 (best 0.618), test acc: 0.576 (best 0.576)
In epoch 25, loss: 0.996, val acc: 0.648 (best 0.648), test acc: 0.626 (best 0.626)
In epoch 30, loss: 0.707, val acc: 0.704 (best 0.704), test acc: 0.671 (best 0.671)
In epoch 35, loss: 0.465, val acc: 0.730 (best 0.730), test acc: 0.714 (best 0.714)
In epoch 40, loss: 0.288, val acc: 0.750 (best 0.750), test acc: 0.749 (best 0.749)
In epoch 45, loss: 0.175, val acc: 0.764 (best 0.764), test acc: 0.756 (best 0.756)
In epoch 50, loss: 0.108, val acc: 0.754 (best 0.764), test acc: 0.755 (best 0.756)
In epoch 55, loss: 0.069, val acc: 0.750 (best 0.764), test acc: 0.756 (best 0.756)
In epoch 60, loss: 0.046, val acc: 0.744 (best 0.764), test acc: 0.757 (best 0.756)
In epoch 65, loss: 0.033, val acc: 0.738 (best 0.764), test acc: 0.759 (best 0.756)
In epoch 70, loss: 0.025, val acc: 0.736 (best 0.764), test acc: 0.758 (best 0.756)
In epoch 75, loss: 0.020, val acc: 0.738 (best 0.764), test acc: 0.759 (best 0.756)
In epoch 80, loss: 0.017, val acc: 0.740 (best 0.764), test acc: 0.761 (best 0.756)
In epoch 85, loss: 0.014, val acc: 0.738 (best 0.764), test acc: 0.762 (best 0.756)
In epoch 90, loss: 0.012, val acc: 0.738 (best 0.764), test acc: 0.762 (best 0.756)
In epoch 95, loss: 0.011, val acc: 0.740 (best 0.764), test acc: 0.761 (best 0.756)
In epoch 100, loss: 0.010, val acc: 0.742 (best 0.764), test acc: 0.761 (best 0.756)
In epoch 105, loss: 0.009, val acc: 0.744 (best 0.764), test acc: 0.761 (best 0.756)
In epoch 110, loss: 0.008, val acc: 0.744 (best 0.764), test acc: 0.762 (best 0.756)
In epoch 115, loss: 0.008, val acc: 0.746 (best 0.764), test acc: 0.763 (best 0.756)
In epoch 120, loss: 0.007, val acc: 0.746 (best 0.764), test acc: 0.763 (best 0.756)
In epoch 125, loss: 0.007, val acc: 0.744 (best 0.764), test acc: 0.763 (best 0.756)
In epoch 130, loss: 0.006, val acc: 0.744 (best 0.764), test acc: 0.762 (best 0.756)
In epoch 135, loss: 0.006, val acc: 0.744 (best 0.764), test acc: 0.763 (best 0.756)
In epoch 140, loss: 0.005, val acc: 0.744 (best 0.764), test acc: 0.763 (best 0.756)
In epoch 145, loss: 0.005, val acc: 0.742 (best 0.764), test acc: 0.763 (best 0.756)
In epoch 150, loss: 0.005, val acc: 0.742 (best 0.764), test acc: 0.764 (best 0.756)
In epoch 155, loss: 0.005, val acc: 0.742 (best 0.764), test acc: 0.765 (best 0.756)
In epoch 160, loss: 0.004, val acc: 0.742 (best 0.764), test acc: 0.765 (best 0.756)
In epoch 165, loss: 0.004, val acc: 0.742 (best 0.764), test acc: 0.765 (best 0.756)
In epoch 170, loss: 0.004, val acc: 0.744 (best 0.764), test acc: 0.766 (best 0.756)
In epoch 175, loss: 0.004, val acc: 0.742 (best 0.764), test acc: 0.767 (best 0.756)
In epoch 180, loss: 0.003, val acc: 0.742 (best 0.764), test acc: 0.767 (best 0.756)
In epoch 185, loss: 0.003, val acc: 0.744 (best 0.764), test acc: 0.767 (best 0.756)
In epoch 190, loss: 0.003, val acc: 0.744 (best 0.764), test acc: 0.767 (best 0.756)
In epoch 195, loss: 0.003, val acc: 0.744 (best 0.764), test acc: 0.767 (best 0.756)
```

## æ›´å¤šçš„ç”¨æˆ·å®šä¹‰å‡½æ•°

DGL å…è®¸ç”¨æˆ·å®šä¹‰messgeå’Œreduceå‡½æ•°ä»¥è·å¾—æœ€å¤§çš„è¡¨ç°åŠ›ã€‚ è¿™æ˜¯ä¸€ä¸ªç”¨æˆ·å®šä¹‰çš„æ¶ˆæ¯å‡½æ•°ï¼Œç›¸å½“äº **fn.u_mul_e('h', 'w', 'm')**ã€‚

```python
def u_mul_e_udf(edges):
    return {'m' : edges.src['h'] * edges.data['w']}
```

`edge` æœ‰ä¸‰ä¸ªæˆå‘˜ï¼š`src`ã€`data `å’Œ`dst`ï¼Œåˆ†åˆ«ä»£è¡¨æ‰€æœ‰è¾¹çš„æºèŠ‚ç‚¹ç‰¹å¾ã€è¾¹ç‰¹å¾å’Œç›®çš„èŠ‚ç‚¹ç‰¹å¾ã€‚

æ‚¨ä¹Ÿå¯ä»¥ç¼–å†™è‡ªå·±çš„reduce å‡½æ•°ã€‚ ä¾‹å¦‚ï¼Œä»¥ä¸‹ç­‰æ•ˆäºå¯¹ä¼ å…¥æ¶ˆæ¯æ±‚å¹³å‡å€¼çš„å†…ç½® **fn.mean('m', 'h_N') å‡½æ•°**ï¼š

```python
def mean_udf(nodes):
    return {'h_N': nodes.mailbox['m'].mean(1)}
```

ç®€è€Œè¨€ä¹‹ï¼ŒDGL å°†æ ¹æ®èŠ‚ç‚¹çš„å…¥åº¦å¯¹èŠ‚ç‚¹è¿›è¡Œåˆ†ç»„ï¼Œå¹¶ä¸”å¯¹äºæ¯ä¸ªç»„ï¼ŒDGL æ²¿ç€ç¬¬äºŒä¸ªç»´åº¦å †å ä¼ å…¥çš„æ¶ˆæ¯ã€‚ ç„¶åï¼Œæ‚¨å¯ä»¥æ²¿ç¬¬äºŒä¸ªç»´åº¦æ‰§è¡Œå½’çº¦ä»¥èšåˆæ¶ˆæ¯ã€‚

> åŸæ–‡ï¼Œè¿™æ®µç¿»è¯‘çš„ä¸æ˜¯å¾ˆå¥½ï¼ŒIn short, DGL will group the nodes by their in-degrees, and for each group DGL stacks the incoming messages along the second dimension. You can then perform a reduction along the second dimension to aggregate messages.

ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°è‡ªå®šä¹‰æ¶ˆæ¯å’Œå‡å°‘å‡½æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [API å‚è€ƒ](https://docs.dgl.ai/api/python/udf.html#apiudf)ã€‚

## ç¼–å†™è‡ªå®šä¹‰ GNN æ¨¡å—çš„æœ€ä½³å®è·µ

DGL æ¨èä»¥ä¸‹ä¼˜å…ˆçº§è”ç³»ï¼š

+ ä½¿ç”¨ **dgl.nn** æ¨¡å—ã€‚
+ ä½¿ç”¨åŒ…å«ä½çº§å¤æ‚æ“ä½œçš„ `dgl.nn.functional `å‡½æ•°ï¼Œä¾‹å¦‚è®¡ç®—ä¼ å…¥è¾¹ä¸Šæ¯ä¸ªèŠ‚ç‚¹çš„ softmaxã€‚
+ å°† **update_all** ä¸å†…ç½®**message**å’Œ**reduce**å‡½æ•°ä¸€èµ·ä½¿ç”¨ã€‚
+ ä½¿ç”¨ç”¨æˆ·å®šä¹‰çš„**message**æˆ–**reduce**å‡½æ•°ã€‚

## Whatâ€™s next?

- [Writing Efficient Message Passing Code](https://docs.dgl.ai/guide/message-efficient.html#guide-message-passing-efficient).

## ä»£ç 

https://docs.dgl.ai/_downloads/f56ecab48bbf4a2401a8e190833eaac7/3_message_passing.py

https://docs.dgl.ai/_downloads/b92a64b54af5c1bb43d3afb760105e4c/3_message_passing.ipynb