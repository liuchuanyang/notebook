# Node Classification with DGL

原文链接：https://docs.dgl.ai/tutorials/blitz/1_introduction.html#overview-of-node-classification-with-gnn

GNN 是许多图形机器学习任务的强大工具。 在本介绍性教程中，您将学习使用 GNN 进行节点分类的基本工作流程，即预测图中节点的类别。

通过完成本教程，您将能够

+ 加载 DGL 提供的数据集。
+ 使用 DGL 提供的神经网络模块构建 GNN 模型。
+ 在 CPU 或 GPU 上训练和评估用于节点分类的 GNN 模型。



本教程假设您有使用 PyTorch 构建神经网络的经验。

（时间估计：13分钟）

```python
import dgl
import torch
import torch.nn as nn
import torch.nn.functional as F
```

# GNN 节点分类概述

图数据上最流行和广泛采用的任务之一是节点分类，其中模型需要预测每个节点的真实类别。在图神经网络之前，许多提议的方法要么单独使用连通性（例如 DeepWalk 或 node2vec），要么使用连通性和节点自身特征的简单组合。相比之下，GNN 提供了通过结合局部邻域的连通性和特征来获得节点表示的机会。

[Kipf 等人](https://arxiv.org/abs/1609.02907)是一个将节点分类问题表述为半监督节点分类任务的例子。仅在一小部分标记节点的帮助下，图神经网络（GNN）就可以准确地预测其他节点的类别。

本教程将展示如何使用 Cora 数据集上的少量标签构建这样一个用于半监督节点分类的 GNN，这是一个以论文为节点、以引文为边的引文网络。任务是预测给定论文的类别。每个论文节点都包含一个词计数向量作为其特征，并进行了归一化处理，使它们的总和为 1，如[论文第 5.2 节](https://arxiv.org/abs/1609.02907)所述。

## 加载 Cora 数据集

```python
import dgl.data

dataset = dgl.data.CoraGraphDataset()
print('Number of categories:', dataset.num_classes)
```

Out:

```
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
Number of categories: 7
```

一个 DGL 数据集对象可能包含一个或多个图形。 本教程中使用的 Cora 数据集仅包含一个图形。

```
g = dataset[0]
```

DGL 图可以将节点特征和边特征存储在两个类似字典的属性中，称为 **ndata** 和**edata**。 在 DGL Cora 数据集中，该图包含以下节点特征：

+ **train_mask** : 指示节点是否在训练集中的布尔张量。
+ **val_mask**: 指示节点是否在验证集中的布尔张量。
+ **test_mask**: 指示节点是否在训练集中的布尔张量。
+ **label**: 节点的真实标签；
+ **feat**:节点的特征；

```python
print('Node features')
print(g.ndata)
print('Edge features')
print(g.edata)
```

Out:

```
Node features
{'feat': tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]), 'label': tensor([3, 4, 4,  ..., 3, 3, 3]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'train_mask': tensor([ True,  True,  True,  ..., False, False, False]), 'val_mask': tensor([False, False, False,  ..., False, False, False])}
Edge features
{}
```

## 定义图卷积网络 (GCN)

本教程将构建一个两层[图卷积网络 (GCN)](http://tkipf.github.io/graph-convolutional-networks/)。 每一层通过聚合邻居信息来计算新的节点表示。

要构建多层 GCN，您可以简单地堆叠 **dgl.nn.GraphConv ** 模块，这些模块继承了 **torch.nn.Module** 。

```python
from dgl.nn import GraphConv

class GCN(nn.Module):
    def __init__(self, in_feats, h_feats, num_classes):
        super(GCN, self).__init__()
        self.conv1 = GraphConv(in_feats, h_feats)
        self.conv2 = GraphConv(h_feats, num_classes)

    def forward(self, g, in_feat):
        h = self.conv1(g, in_feat)
        h = F.relu(h)
        h = self.conv2(g, h)
        return h

# Create the model with given dimensions
model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)
```

DGL 提供了许多流行的邻居聚合模块的实现。 您可以使用一行代码轻松调用它们。

## 训练 GCN

训练这个 GCN 类似于训练其他 PyTorch 神经网络。

```python
def train(g, model):
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    best_val_acc = 0
    best_test_acc = 0

    features = g.ndata['feat']
    labels = g.ndata['label']
    train_mask = g.ndata['train_mask']
    val_mask = g.ndata['val_mask']
    test_mask = g.ndata['test_mask']
    for e in range(100):
        # Forward
        logits = model(g, features)

        # Compute prediction
        pred = logits.argmax(1)

        # Compute loss
        # Note that you should only compute the losses of the nodes in the training set.
        loss = F.cross_entropy(logits[train_mask], labels[train_mask])

        # Compute accuracy on training/validation/test
        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()
        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()
        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()

        # Save the best validation accuracy and the corresponding test accuracy.
        if best_val_acc < val_acc:
            best_val_acc = val_acc
            best_test_acc = test_acc

        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if e % 5 == 0:
            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(
                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))
model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)
train(g, model)
```

Out:

```
In epoch 0, loss: 1.946, val acc: 0.194 (best 0.194), test acc: 0.208 (best 0.208)
In epoch 5, loss: 1.887, val acc: 0.622 (best 0.622), test acc: 0.659 (best 0.659)
In epoch 10, loss: 1.800, val acc: 0.652 (best 0.674), test acc: 0.682 (best 0.710)
In epoch 15, loss: 1.689, val acc: 0.668 (best 0.674), test acc: 0.697 (best 0.710)
In epoch 20, loss: 1.555, val acc: 0.700 (best 0.700), test acc: 0.725 (best 0.725)
In epoch 25, loss: 1.400, val acc: 0.710 (best 0.712), test acc: 0.736 (best 0.732)
In epoch 30, loss: 1.229, val acc: 0.730 (best 0.730), test acc: 0.746 (best 0.746)
In epoch 35, loss: 1.052, val acc: 0.726 (best 0.730), test acc: 0.765 (best 0.746)
In epoch 40, loss: 0.878, val acc: 0.744 (best 0.744), test acc: 0.767 (best 0.767)
In epoch 45, loss: 0.717, val acc: 0.752 (best 0.752), test acc: 0.767 (best 0.766)
In epoch 50, loss: 0.576, val acc: 0.756 (best 0.756), test acc: 0.771 (best 0.772)
In epoch 55, loss: 0.459, val acc: 0.770 (best 0.770), test acc: 0.773 (best 0.773)
In epoch 60, loss: 0.365, val acc: 0.778 (best 0.778), test acc: 0.772 (best 0.772)
In epoch 65, loss: 0.292, val acc: 0.770 (best 0.778), test acc: 0.777 (best 0.772)
In epoch 70, loss: 0.235, val acc: 0.772 (best 0.778), test acc: 0.776 (best 0.772)
In epoch 75, loss: 0.191, val acc: 0.772 (best 0.778), test acc: 0.781 (best 0.772)
In epoch 80, loss: 0.157, val acc: 0.772 (best 0.778), test acc: 0.781 (best 0.772)
In epoch 85, loss: 0.131, val acc: 0.774 (best 0.778), test acc: 0.781 (best 0.772)
In epoch 90, loss: 0.111, val acc: 0.770 (best 0.778), test acc: 0.780 (best 0.772)
In epoch 95, loss: 0.094, val acc: 0.770 (best 0.778), test acc: 0.777 (best 0.772)
```

## 在 GPU 上训练

在 GPU 上训练需要使用 **to** 方法将模型和图形都放到 GPU 上，类似于您将在 PyTorch 中执行的操作。

```python
g = g.to('cuda')
model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes).to('cuda')
train(g, model)
```

## What's next?

- [How does DGL represent a graph](https://docs.dgl.ai/tutorials/blitz/2_dglgraph.html)?
- [Write your own GNN module](https://docs.dgl.ai/tutorials/blitz/3_message_passing.html).
- [Link prediction (predicting existence of edges) on full graph](https://docs.dgl.ai/tutorials/blitz/4_link_predict.html).
- [Graph classification](https://docs.dgl.ai/tutorials/blitz/5_graph_classification.html).
- [Make your own dataset](https://docs.dgl.ai/tutorials/blitz/6_load_data.html).
- [The list of supported graph convolution modules](https://docs.dgl.ai/api/python/nn.pytorch.html#apinn-pytorch).
- [The list of datasets provided by DGL](https://docs.dgl.ai/api/python/dgl.data.html#apidata).

## 代码

https://docs.dgl.ai/_downloads/f0f86f5ebce0432cf782aa916797594c/1_introduction.py

https://docs.dgl.ai/_downloads/b8c6bed6f07efe8046e874c957b27f7a/1_introduction.ipynb