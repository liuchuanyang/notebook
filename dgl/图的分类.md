## 训练用于图分类的 GNN

在本教程结束时，您将能够

+ 加载 DGL 提供的图分类数据集。
+ 了解读出函数的作用。
+ 了解如何创建和使用小批量图。
+ 构建基于 GNN 的图分类模型。
+ 在 DGL 提供的数据集上训练和评估模型。

（时间估计：18分钟）

```python
import dgl
import torch
import torch.nn as nn
import torch.nn.functional as F
```

## 使用 GNN 进行图分类概述

图分类或回归需要一个模型来预测单个图的特定图级属性，给定其节点和边特征。 分子特性预测是一种特殊的应用。

本教程展示了如何为来自论文[图神经网络有多强大](https://arxiv.org/abs/1810.00826)的小数据集训练图分类模型。

## 加载数据

```python
import dgl.data

# Generate a synthetic dataset with 10000 graphs, ranging from 10 to 500 nodes.
dataset = dgl.data.GINDataset('PROTEINS', self_loop=True)
```

数据集是一组图，每个图都有节点特征和单个标签。 可以在 `dim_nfeats `和 `gclasses `属性中看到节点特征维数和 `GINDataset `对象的可能图形类别数。

```python
print('Node feature dimensionality:', dataset.dim_nfeats)
print('Number of graph categories:', dataset.gclasses)
```

Out:

> Node feature dimensionality: 3
> Number of graph categories: 2

## 定义数据加载器

图分类数据集通常包含两种类型的元素：一组图及其图级标签。 类似于图像分类任务，当数据集足够大时，我们需要使用小批量进行训练。 当您训练用于图像分类或语言建模的模型时，您将使用 `DataLoader `迭代数据集。 在 DGL 中，您可以使用 `GraphDataLoader`。

您还可以使用 [torch.utils.data.sampler](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler) 中提供的各种数据集采样器。 例如，本教程创建一个训练 `GraphDataLoader` 并测试 `GraphDataLoader`，使用` SubsetRandomSampler` 告诉 PyTorch 仅从数据集的一个子集中进行采样。

```python
from dgl.dataloading import GraphDataLoader
from torch.utils.data.sampler import SubsetRandomSampler

num_examples = len(dataset)
num_train = int(num_examples * 0.8)

train_sampler = SubsetRandomSampler(torch.arange(num_train))
test_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))

train_dataloader = GraphDataLoader(
    dataset, sampler=train_sampler, batch_size=5, drop_last=False)
test_dataloader = GraphDataLoader(
    dataset, sampler=test_sampler, batch_size=5, drop_last=False)
```

您可以尝试遍历创建的 `Graph DataLoader` 并查看它提供了什么：

```python
it = iter(train_dataloader)
batch = next(it)
print(batch)
```

Out:

> [Graph(num_nodes=186, num_edges=926,
>       ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float64), 'label': Scheme(shape=(), dtype=torch.int64)}
>       edata_schemes={}), tensor([1, 1, 1, 0, 0])]

由于`dataset`中的每个元素都有一个图和一个标签，`GraphDataLoader `将在每次迭代中返回两个对象。 第一个元素是批处理图，第二个元素只是一个标签向量，表示小批量中每个图的类别。 接下来，我们将讨论批处理图。

## DGL 中的批处理图

在每个小批量中，采样图通过 `dgl.batch` 组合成一个更大的批处理图。 单个更大的批处理图将所有原始图合并为单独连接的组件，并连接节点和边特征。 这个更大的图也是一个` DGLGraph` 实例（所以你仍然可以像[这里](https://docs.dgl.ai/tutorials/blitz/2_dglgraph.ipynb)一样把它当作一个普通的 DGLGraph 对象）。 然而，它包含恢复原始图所必需的信息，例如每个图元素的节点和边的数量。

```python
batched_graph, labels = batch
print('Number of nodes for each graph element in the batch:', batched_graph.batch_num_nodes())
print('Number of edges for each graph element in the batch:', batched_graph.batch_num_edges())

# Recover the original graph elements from the minibatch
graphs = dgl.unbatch(batched_graph)
print('The original graphs in the minibatch:')
print(graphs)
```

Out:

> Number of nodes for each graph element in the batch: tensor([21,  9, 27, 90, 39])
> Number of edges for each graph element in the batch: tensor([107,  39, 141, 464, 175])
> The original graphs in the minibatch:
> [Graph(num_nodes=21, num_edges=107,
>       ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float64), 'label': Scheme(shape=(), dtype=torch.int64)}
>       edata_schemes={}), Graph(num_nodes=9, num_edges=39,
>       ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float64), 'label': Scheme(shape=(), dtype=torch.int64)}
>       edata_schemes={}), Graph(num_nodes=27, num_edges=141,
>       ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float64), 'label': Scheme(shape=(), dtype=torch.int64)}
>       edata_schemes={}), Graph(num_nodes=90, num_edges=464,
>       ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float64), 'label': Scheme(shape=(), dtype=torch.int64)}
>       edata_schemes={}), Graph(num_nodes=39, num_edges=175,
>       ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float64), 'label': Scheme(shape=(), dtype=torch.int64)}
>       edata_schemes={})]

## 定义模型

本教程将构建一个两层[图卷积网络 (GCN)](http://tkipf.github.io/graph-convolutional-networks/)。 它的每一层都通过聚合邻居信息来计算新的节点表示。 如果你看过[介绍](https://docs.dgl.ai/tutorials/blitz/1_introduction.html)，你会注意到两个不同之处：

+ 由于任务是为整个图而不是每个节点预测单个类别，因此您需要聚合所有节点的表示以及可能的边以形成图级表示。 这种过程通常称为读出。 一个简单的选择是使用 `dgl.mean_nodes() `对图的节点特征进行平均。
+ 模型的输入图将是由 `GraphDataLoader` 生成的批处理图。 DGL 提供的读出函数可以处理批处理图，以便它们为每个小批量元素返回一个表示。

```python
from dgl.nn import GraphConv

class GCN(nn.Module):
    def __init__(self, in_feats, h_feats, num_classes):
        super(GCN, self).__init__()
        self.conv1 = GraphConv(in_feats, h_feats)
        self.conv2 = GraphConv(h_feats, num_classes)

    def forward(self, g, in_feat):
        h = self.conv1(g, in_feat)
        h = F.relu(h)
        h = self.conv2(g, h)
        g.ndata['h'] = h
        return dgl.mean_nodes(g, 'h')
```

## 训练

训练循环使用` GraphDataLoader `对象迭代训练集并计算梯度，就像图像分类或语言建模一样。

```python
# Create the model with given dimensions
model = GCN(dataset.dim_nfeats, 16, dataset.gclasses)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

for epoch in range(20):
    for batched_graph, labels in train_dataloader:
        pred = model(batched_graph, batched_graph.ndata['attr'].float())
        loss = F.cross_entropy(pred, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

num_correct = 0
num_tests = 0
for batched_graph, labels in test_dataloader:
    pred = model(batched_graph, batched_graph.ndata['attr'].float())
    num_correct += (pred.argmax(1) == labels).sum().item()
    num_tests += len(labels)

print('Test accuracy:', num_correct / num_tests)
```

Out:

> Test accuracy: 0.22869955156950672

## What's next?

- See [GIN example](https://github.com/dmlc/dgl/tree/master/examples/pytorch/gin) for an end-to-end graph classification model.

## 代码

https://docs.dgl.ai/_downloads/aaa2a0e39989a57e0abe6fd332fdd062/5_graph_classification.py

https://docs.dgl.ai/_downloads/e755bd465751e8eb65cb802f5b94910c/5_graph_classification.ipynb